{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 999\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from wordcloud import WordCloud\n",
    "import pandas_profiling\n",
    "import missingno as msno\n",
    "import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ycr/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/ycr/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "twitter1 = pd.read_csv(\"trayvontweets/Trayvon Tweets 2012.csv\",encoding = \"latin1\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter2 = pd.read_csv(\"trayvontweets/Trayvon Tweets 2013.csv\",encoding = \"latin1\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(dates):\n",
    "    delta=datetime.timedelta(days=dates)\n",
    "    today=datetime.datetime.strptime('1899-12-30 00:00:00','%Y-%m-%d %H:%M:%S')+delta\n",
    "    return datetime.datetime.strftime(today,'%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = pd.concat([twitter1,twitter2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2097150 entries, 0 to 2097149\n",
      "Data columns (total 20 columns):\n",
      "id                      int64\n",
      "message_id              int64\n",
      "user_handle             object\n",
      "orig_user_id            int64\n",
      "orig_user_handle        object\n",
      "topsy_type              object\n",
      "orig_date_posted_gmt    float64\n",
      "date_posted_gmt         float64\n",
      "message                 object\n",
      "orig_user_location      object\n",
      "coordinates             object\n",
      "tweet_location          object\n",
      "longitude               float64\n",
      "latitude                float64\n",
      "created_at_gmt          float64\n",
      "seconds_after_event     float64\n",
      "FIPS                    object\n",
      "County                  object\n",
      "City                    object\n",
      "State                   object\n",
      "dtypes: float64(6), int64(3), object(11)\n",
      "memory usage: 320.0+ MB\n"
     ]
    }
   ],
   "source": [
    "twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.sample(n = 1000)\n",
    "twitter = twitter.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.loc[:,'message'] = twitter.loc[:,'message'].fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter = twitter.drop('index',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['retweet:native', 'tweet', 'retweet:reply'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.loc[:,'topsy_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to output twitter message as txt\n",
    "#twitter_message = pd.DataFrame(twitter_message)\n",
    "#twitter_message = twitter_message.reset_index()\n",
    "#twitter_message.to_csv(\"twitter_message\",sep = \"\\t\",index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUnicode(text):\n",
    "    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\", this mothod will remove all emojis\"\"\"   \n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceURL(text):\n",
    "    \"\"\" remove url address\"\"\"\n",
    "    text = re.sub('((www\\.[^\\s]+)|(http?://[^\\s]+))','',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceAtUser(text):\n",
    "    \"\"\" remove\"@user\" \"\"\"\n",
    "    text = re.sub('@[^\\s]+','',text) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeHashtagInFrontOfWord(text):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)  #only remove# or remove all?\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeRT(text):\n",
    "    \"\"\" Removes hastag in front of a word \"\"\"\n",
    "    text = re.sub(r'\\bRT\\b', r'', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(text):\n",
    "    \"\"\"remove Punctuations\"\"\"\n",
    "    text = re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=\\&\\(\\)\\\"]\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_map = {}\n",
    "with open('contractions.txt') as file:\n",
    "    for line in file:\n",
    "        (key,val)= line.rstrip().split(\":\")\n",
    "        contraction_map[key] = val\n",
    "        \n",
    "def replaceContractions(word):\n",
    "    \"\"\"Remove contractions \"\"\"\n",
    "    if word in contraction_map.keys():\n",
    "        return contraction_map[word].split()\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('slang.txt') as file:\n",
    "    slang_map = dict(map(str.strip, line.partition('\\t')[::2]) for line in file if line.strip())\n",
    "\n",
    "slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spell correction\n",
    "#correct I to a, but keep i\n",
    "#need more work on it\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('corporaForSpellCorrection.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"\"\"P robability of `word`. \"\"\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def spellCorrection(word): \n",
    "    \"\"\" Most probable spelling correction for word. \"\"\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"\"\" Generate possible spelling corrections for word. \"\"\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"\"\" The subset of `words` that appear in the dictionary of WORDS. \"\"\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"\"\" All edits that are one edit away from `word`. \"\"\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"\"\" All edits that are two edits away from `word`. \"\"\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use bag model or sequence model?\n",
    "def replace(word, pos=None):\n",
    "    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n",
    "    antonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos):\n",
    "        for lemma in syn.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                antonyms.add(antonym.name())\n",
    "    if len(antonyms) == 1:\n",
    "        return antonyms.pop()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def replaceNegations(text):\n",
    "    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n",
    "    i, l = 0, len(text)\n",
    "    words = []\n",
    "    while i < l:\n",
    "        word = text[i]\n",
    "        if word == 'not' and i+1 < l:\n",
    "            ant = replace(text[i+1])\n",
    "            if ant:\n",
    "                words.append(ant)\n",
    "                i += 2\n",
    "                continue\n",
    "\n",
    "        words.append(word)\n",
    "        i += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "multi_tokenizer = MWETokenizer()\n",
    "multi_tokenizer.add_mwe(('b', 'l', 'm'))\n",
    "multi_tokenizer.add_mwe(('r', 'i', 'p')) #add by hand?\n",
    "twitter_tokens = []\n",
    "twitter_token_cleaned = []\n",
    "for i in range(twitter.shape[0]):\n",
    "    twitter.loc[i,'message_cleaned'] = removeUnicode(twitter.loc[i,'message'])\n",
    "    twitter.loc[i,'message_cleaned'] = replaceURL(twitter.loc[i,'message_cleaned'])\n",
    "    twitter.loc[i,'message_cleaned'] = replaceAtUser(twitter.loc[i,'message_cleaned'])    \n",
    "    twitter.loc[i,'message_cleaned'] = removeHashtagInFrontOfWord(twitter.loc[i,'message_cleaned'])\n",
    "    twitter.loc[i,'message_cleaned'] = removeRT(twitter.loc[i,'message_cleaned'])\n",
    "    twitter.loc[i,'message_cleaned'] = replaceSlang(twitter.loc[i,'message_cleaned'])\n",
    "    twitter.loc[i,'message_cleaned'] = removePunctuations(twitter.loc[i,'message_cleaned'])\n",
    "    twitter.loc[i,'message_cleaned'] = twitter.loc[i,'message_cleaned'].lower()\n",
    "    twitter_tokens.append(multi_tokenizer.tokenize(tokenizer.tokenize(twitter.loc[i,'message_cleaned'])))\n",
    "    \n",
    "    #replace contraction and spell correction begins\n",
    "    for j in range(0,len(twitter_tokens[i])):\n",
    "        twitter_tokens[i][j] = spellCorrection(twitter_tokens[i][j])   #spell correction will take a long time\n",
    "        twitter_tokens[i][j] = replaceContractions(twitter_tokens[i][j])\n",
    "    for j in range(0,len(twitter_tokens[i])):\n",
    "        twitter_token_cleaned.append([])\n",
    "        if isinstance(twitter_tokens[i][j],list):\n",
    "            for item in twitter_tokens[i][j]:\n",
    "                twitter_token_cleaned[i].append(item)\n",
    "        else:\n",
    "            twitter_token_cleaned[i].append(twitter_tokens[i][j])\n",
    "    #replace contraction and spell correction ends\n",
    "    \n",
    "    twitter_token_cleaned[i] = replaceNegations(twitter_token_cleaned[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zimmermannnunderage'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tokens[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:    \n",
    "**twitter_token_cleaned**:  list of list of words         \n",
    "twitter_token_cleaned[0] is all the words of row0 in twitter.            \n",
    "delete all punctuation, @user, url, hashtags; lower case, deal with negation, spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nan in message:      \n",
    "is it all \"reply\"?\n",
    "- negation    \n",
    "we use: find \"not\" and replace with antonym    \n",
    "maybe sequential model?     \n",
    "or do not deal with negations?\n",
    "- emojis      \n",
    "how to read emojis and keep the unicodes?     \n",
    "https://unicode.org/emoji/charts/full-emoji-list.html#1f600     \n",
    "https://anaconda.org/conda-forge/emoji     \n",
    "- get all hashtags? create a new column?\n",
    "- special words will be tokenized, like R.I.P, using mwetokenizer, add by hands\n",
    "- count upper case or ignore them(convert to lower case)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read LIWC dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (capstone)",
   "language": "python",
   "name": "capstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
